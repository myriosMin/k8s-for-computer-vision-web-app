# xBD with YOLO + K8S

## Project Directory

```
.
â”œâ”€â”€ Makefile                  # Automation commands for build, deploy, jobs
â”œâ”€â”€ README.md                 # Project overview and instructions
â”œâ”€â”€ Xview2_yolo.ipynb         # Model consideration & development; done in colab - Min
â”œâ”€â”€ commit.md                 # Cleaned git logs (Contribution table)
â”œâ”€â”€ commit.txt                # Raw git logs
â”œâ”€â”€ docker-cmd-sketch.txt     # Draft Docker run commands
â”œâ”€â”€ data                      # Datasets and YOLO-formatted data; put aiad_data.zip and test.zip here
â”‚Â Â  â”œâ”€â”€ aiad_data.zip         # Raw dataset archive
â”‚Â Â  â”œâ”€â”€ test.zip              # Test dataset archive
â”‚Â Â  â””â”€â”€ yolo_xbd              # Preprocessed YOLO data; contents are generated by code
â”‚Â Â      â”œâ”€â”€ images            # train/val/test image folders
â”‚Â Â      â”œâ”€â”€ labels            # train/val/test annotation files
â”‚Â Â      â”œâ”€â”€ test_pairs.csv    # Pre/post image mapping for test set
â”‚Â Â      â”œâ”€â”€ train_pairs.csv   # Pre/post image mapping for training set
â”‚Â Â      â”œâ”€â”€ val_pairs.csv     # Pre/post image mapping for validation set
â”‚Â Â      â””â”€â”€ xbd6.yaml         # YOLO dataset configuration
â”œâ”€â”€ images                    # Docker image source codes and build contexts
â”‚Â Â  â”œâ”€â”€ infer/infer-cpu       # Inference API container
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ app/infer_api.py  # FAST API
â”‚Â Â  â”‚Â Â  â””â”€â”€ requirements.txt
â”‚Â Â  â”œâ”€â”€ ui/ui-cpu             # UI container
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ app/app.py        # Flask backend
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ static/css        # Tailwind config & styles
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ static/js/app.js  # Frontend JS
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ templates         # HTML templates
â”‚Â Â  â”‚Â Â  â””â”€â”€ wsgi.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ requirements.txt
â”‚Â Â  â””â”€â”€ worker/worker-cpu     # Preprocessing & training container
â”‚Â Â      â”œâ”€â”€ Dockerfile
â”‚Â Â      â”œâ”€â”€ app/common.py     # Common datasets
â”‚Â Â      â”œâ”€â”€ app/preprocess.py # Preprocessing from xBD to YOLO format
â”‚Â Â      â”œâ”€â”€ app/train.py      # YOLO multi-class detection training
â”‚Â Â      â””â”€â”€ requirements.txt
â”œâ”€â”€ k8s                       # Kubernetes manifests & kustomize configs
â”‚Â Â  â”œâ”€â”€ base                  # Base configs
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ configmap-app.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ deploy-infer.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ deploy-ui.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hpa-infer.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hpa-ui.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ job-preprocess.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ job-train.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ kustomization.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ namespace.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pvc-datasets.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pvc-models.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pvc-outputs.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ secret-app.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ svc-infer.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ svc-ui.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ netpol-default-deny.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ netpol-allow-ui-infer.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ netpol-engress-web.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ netpol-egress-dns.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ rbac-ui.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ allow-preprocess-to-minio.yaml     # not-in-use
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ allow-ui-configmap-patch.yaml      # not-in-use
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ allow-ui-ingress.yaml              # not-in-use
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ minio-deployment.yaml              # not-in-use; intended for centralized persistent storage
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ minio-namespace.yaml               # not-in-use
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ minio-pvc.yaml                     # not-in-use
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ minio-secret.yaml                  # not-in-use
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ minio-service.yaml                 # not-in-use
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pdb-infer.yaml                     # not-in-use; intended for load distribution
â”‚Â Â  â”‚Â Â  â””â”€â”€ pdb-ui.yaml                        # not-in-use; intended for load distribution 
â”‚Â Â  â”œâ”€â”€ cpu                                    # Cpu overlay configs
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ kustomization-infrastructure.yaml  # pvc and configs
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ kustomization-services.yaml        # infer, ui
â”‚Â Â  |   â””â”€â”€ kustomization.yaml
â”‚Â Â  â””â”€â”€ dev                                    # Dev overlay configs
â”‚Â Â      â””â”€â”€ kustomization.yaml
â”œâ”€â”€ models                                     # Trained model weights; put best.pt here
â”‚Â Â  â””â”€â”€ best.pt
â”œâ”€â”€ output                                     # Inference results and processed output
â”‚Â Â  â”œâ”€â”€ predictions                            # Real-time inferences
â”‚Â Â  â””â”€â”€ yolo_xbd_6ch                           # Model training outputs
â””â”€â”€ scripts                                    # Development utility scripts
    â”œâ”€â”€ deploy_cpu.sh
    â”œâ”€â”€ dev_minikube_env.sh
    â””â”€â”€ tailwind_build.sh
```
> ðŸ“ƒ Note for lecturer: Please also check out min-cpu branch. I tried out Canary model rollout deployment; it works, but it's difficult to test in our limited set-up, and so, I decided not to demo it during presentation. By right, if a new model is fine-tuned in UI, instead of it instantly replacing old one, like what it currently does, there will be two models serving at the same time: old model handling half the traffic and new model the other half. If there is no error and new model runs successful for the next 3 minutes (accuracy is usually monitored in actual deployment, but I just put a timer to point out the feasability), it will replace the old one. If not, old one remains. The old model remains in version control for roll back if necessary.

> Some old, un-used files are also not cleaned up in purpose since they reflect our effort in debugging and brain-storming together. 

# Quickstart (Essentials)

### What You Need to Fill In

* Put `aiad_data.zip` under `/data` which includes raw xBD data, partially or full.

* Adjust epochs and pvc sizes as per your intended data input and storage restrictions.

### Pre-installations 

* Make sure Docker, Minikube, and Make (for Windows) are installed.
* Put `best.pt` under `/models` as the base model weights.
* Start minikube (with ingress and metrics-server addons, if not enabled yet):
```bash
minikube start
minikube addons enable ingress
minikube addons enable metrics-server
```

### 0. One-Shot Setup (Recommended)

For a lightweight, CPU-only deployment with minimal resource requirements (perfect for testing and development); it builds the docker images inside minikube, copy base model to pvc, deploy the k8s components, then serve:

```bash
make cpu-all
```

**Benefits of CPU deployment:**
- **Faster builds**: No CUDA dependencies, smaller images (< 2GB for all images)
- **Minimal storage**: 2Gi datasets, 1Gi models, 1Gi outputs (vs 20Gi/5Gi/10Gi)
- **Lower resources**: 50m CPU UI, 100m CPU inference (vs 100m/250m)
- **Quick iteration**: Perfect for development and CI/CD (and academic marking ^^)
- **Cost effective**: Runs on any machine with minikube

See `k8s/cpu/README.md` for detailed CPU deployment documentation.

> âš ï¸ Note: Train the model for 1 epoch with 1-2 batch size only since 6-channel yolo model training on CPU is quite resource intensive. Increase the resource limits in `k8s/base/job-train.yaml`. (Current limit took ~5 minutes on Intel i7-12700H.)

> Follow the below step-by-step deployment if you did not run `make cpu-all`, or it failed.

---

### 1. Start Minikube and Enable Addons

```bash
make up
make addons
```

### 2. Build Images Inside Minikube's Docker

**For CPU-only lightweight builds:**
```bash
make build-cpu
```

### 3. Deploy Everything (Namespace, PVCs, UI, Inference, Ingress)

**For CPU-only lightweight deployment:**
```bash
make deploy-cpu
```

### 4. Add Host Entry

Usually unnecessary: `app.localtest.me` resolves to `127.0.0.1` already.

### 5. Open UI

```bash
make ui
```
then browse http://app.localtest.me, OR expose the url with minikube
```bash
minikube service ui -n xview
```

### 6. Upload and Train

Upload raw **xBD** files (aiad_data.zip) on the **"Data & Training"** page in UI. Then run (preferably for manageable epoch and batch size for testing):

```bash
make job-preprocess
make job-train   # trains, saves runs and best.pt under /models
```

### 7. Link Trained Weights to Inference Deployment

Inference uses a fixed `/models/best.pt`. If itâ€™s missing, it pulls a base `yolo11n-seg` model (for initial first-time training only; current set-up is fine-tuning, and you must put `best.pt` under `models` before any make commands).

```bash
kubectl -n xview rollout restart deploy/infer
```

### 8. Run Inference from UI

On the **"Inference"** page, upload a single image. The UI calls `infer:9000/predict` and displays detections with an overlay. 

You can also upload a zipped folder (test.zip) of pre- and post- disaster images with matching names for batch inference; you will see a downloadable link, when done, inside which are output images with predicted bounding boxes.

---

### Notes on GPU in Minikube

* If using Windows + WSL2 + Docker Desktop with NVIDIA enabled:

  * Enable GPU in Docker Desktop: *Settings â†’ Resources â†’ GPU*
  * Start Minikube with GPU:

    ```bash
    minikube start --driver=docker --gpu
    ```
  * Install NVIDIA device plugin (optional; may be flaky in Minikube)
* If no GPU, training jobs ignore GPU limits and fall back to CPU (`torch.cuda.is_available()` check in place). *Consider reducing epochs in configmap-app.yaml for testing purposes*.

---
